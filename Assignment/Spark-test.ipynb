{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb39875",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Please answer all questions\n",
    "- You can use any language you wish (e.g. Python, Scala, SQL...)\n",
    "- Several Markdown cells require completion. Please edit the Markdown cells to include your answer.\n",
    "- Your final notebook should compile without errors when you click \"Run All\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaca273",
   "metadata": {},
   "source": [
    "## Part 1: Reading and Parsing Data\n",
    "\n",
    "### Question 1:  Code Challenge - Load a CSV\n",
    "\n",
    "- Load the CSV file at `treecover.csv` into a DataFrame.\n",
    "- Use Apache Spark to read in the data, assigned to the variable `treeCoverDF`.\n",
    "- Please use the `inferSchema` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "795de1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Hands-on-Assignment-PySpark\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"target/spark-warehouse\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1d228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n",
      "| Id|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Horizontal_Distance_To_Fire_Points|Cover_Type|Soil_Type|Wilderness_Area|Hillshade|\n",
      "+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n",
      "|  1|     2596|    51|    3|                             258|                             0|                            510|                              6279|         5|       29|              1|      9am|\n",
      "|  2|     2590|    56|    2|                             212|                            -6|                            390|                              6225|         5|       29|              1|     Noon|\n",
      "+---+---------+------+-----+--------------------------------+------------------------------+-------------------------------+----------------------------------+----------+---------+---------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treeCoverDF = spark.read.csv('treecover.csv', inferSchema=True, header=True)\n",
    "treeCoverDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa348bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'Elevation',\n",
       " 'Aspect',\n",
       " 'Slope',\n",
       " 'Horizontal_Distance_To_Hydrology',\n",
       " 'Vertical_Distance_To_Hydrology',\n",
       " 'Horizontal_Distance_To_Roadways',\n",
       " 'Horizontal_Distance_To_Fire_Points',\n",
       " 'Cover_Type',\n",
       " 'Soil_Type',\n",
       " 'Wilderness_Area',\n",
       " 'Hillshade']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeCoverDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d8a4e",
   "metadata": {},
   "source": [
    "### Question 2:  Code Challenge - Print the Schema\n",
    "\n",
    "Use Apache Spark to display the Schema of the `treeCoverDF` Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99768dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Cover_Type: integer (nullable = true)\n",
      " |-- Soil_Type: integer (nullable = true)\n",
      " |-- Wilderness_Area: integer (nullable = true)\n",
      " |-- Hillshade: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "treeCoverDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53ce3c",
   "metadata": {},
   "source": [
    "### Question 3:  Code Challenge - Rows & Columns\n",
    "\n",
    "Use Apache Spark to display the number of rows and columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4a09982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows    = 15120\n",
      "No. of Columns = 12\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of rows    = {}\".format(treeCoverDF.count()))\n",
    "print(\"No. of Columns = {}\".format(len(treeCoverDF.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced144a",
   "metadata": {},
   "source": [
    "#Part 2: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ccc49",
   "metadata": {},
   "source": [
    "### Question 4:  Code Challenge - Summary Statistics for a Feature\n",
    "\n",
    "Use Apache Spark to answer these questions about the `treeCoverDF` DataFrame:\n",
    "- What is the range - minimum and maximum - of values for the feature `elevation`?\n",
    "- What are the mean and standard deviation of the feature `elevation`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a405978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min elevation : 1863.0\n",
      "Max elevation : 3849.0\n",
      "Mean elevation : 2749.3225529100528\n",
      "Standard Deviation of elevation : 417.67818734804985\n"
     ]
    }
   ],
   "source": [
    "#1 way\n",
    "\n",
    "summary_table = treeCoverDF.select('Elevation').describe()\n",
    "#summary_table.show()\n",
    "print(\"Min elevation :\",float(summary_table.filter(col('summary') == 'min').first().asDict()['Elevation']))\n",
    "print(\"Max elevation :\",float(summary_table.filter(col('summary') == 'max').first().asDict()['Elevation']))\n",
    "print(\"Mean elevation :\",float(summary_table.filter(col('summary') == 'mean').first().asDict()['Elevation']))\n",
    "print(\"Standard Deviation of elevation :\",float(summary_table.filter(col('summary') == 'stddev').first().asDict()['Elevation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "244d87bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(elevation)|max(elevation)|\n",
      "+--------------+--------------+\n",
      "|          1863|          3849|\n",
      "+--------------+--------------+\n",
      "\n",
      "+------------------+----------------------+\n",
      "|    avg(elevation)|stddev_samp(elevation)|\n",
      "+------------------+----------------------+\n",
      "|2749.3225529100528|    417.67818734804985|\n",
      "+------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 way\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "treeCoverDF.agg(min(col(\"elevation\")), max(col(\"elevation\"))).show()\n",
    "\n",
    "treeCoverDF.agg(mean(col(\"elevation\")), stddev(col(\"elevation\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730c36a",
   "metadata": {},
   "source": [
    "### Answer #4:\n",
    "\n",
    "- Min `elevation`: `YOUR ANSWER HERE`\n",
    "- Max `elevation`: `YOUR ANSWER HERE`\n",
    "- Mean `elevation`: `YOUR ANSWER HERE`\n",
    "- Standard Deviation of `elevation`: `YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34262c",
   "metadata": {},
   "source": [
    "### Question 5:  Code Challenge - Record Count\n",
    "\n",
    "Use Apache Spark to answer the following question:\n",
    "- How many entries in the dataset have an `elevation` greater than or equal to 2749.32 meters **AND** a `Cover_Type` of 1 or 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b98b1ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeCoverDF.filter((col('elevation') >= 2749.32) & (col('Cover_Type').isin([1,2]))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9056ec18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4279"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeCoverDF.filter('elevation >= 2749.32 and cover_type = 1 or cover_type = 2').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2fba74",
   "metadata": {},
   "source": [
    "### Question 6: Code Challenge - Compute a Percentage\n",
    "\n",
    "Use Apache Spark to answer the following question:\n",
    "- What percentage of entries with `Cover_Type` 1 or 2 have an `elevation` at or above 2749.32 meters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba4de4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_count = treeCoverDF.select('Elevation').filter(((col('Cover_Type') == 1) | (col('Cover_Type') == 2)) & (col('Elevation') >= 2749.32)).count()\n",
    "filt_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae895b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4279"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtercount = treeCoverDF.filter('elevation >= 2749.32 and cover_type = 1 or cover_type = 2').count()\n",
    "filtercount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a5eea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.681216931216934\n",
      "28.300264550264547\n"
     ]
    }
   ],
   "source": [
    "totalcount = treeCoverDF.count()\n",
    "print((filt_count/totalcount)*100)\n",
    "\n",
    "print((filtercount/totalcount)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636492f",
   "metadata": {},
   "source": [
    "### Question 7: Code Challenge - Visualize Feature Distribution\n",
    "\n",
    "\n",
    "- a bar chart that helps visualize the distribution of different Wilderness Areas in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b58a3f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Wilderness_Area: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(treeCoverDF.select('Wilderness_Area').distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c11ca",
   "metadata": {},
   "source": [
    "### Question 8: Code Challenge - Visualize Average Elevation by Cover Type \n",
    "\n",
    "- a bar chart showing the average elevation of each cover type with string labels for cover type\n",
    "\n",
    "**NOTE: you will need to match the integer values in the column `treeCoverDF.Cover_Type` to the string values in `dbfs:/FileStore/tmp/nl/covertype.csv` to retrieve the Cover Type Labels. It is recommended to use an Apache Spark join.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "461daac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|cover_type_key| cover_type_label|\n",
      "+--------------+-----------------+\n",
      "|             1|       Spruce/Fir|\n",
      "|             2|   Lodgepole Pine|\n",
      "|             3|   Ponderosa Pine|\n",
      "|             4|Cottonwood/Willow|\n",
      "|             5|            Aspen|\n",
      "|             6|      Douglas-fir|\n",
      "|             7|        Krummholz|\n",
      "+--------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[elevation: int, cover_type_label: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# way 1\n",
    "\n",
    "treeCoverType = spark.read.load(\"covertype.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "treeCoverType.show()   \n",
    "\n",
    "newdf = treeCoverDF.join(treeCoverType,treeCoverDF.Cover_Type == treeCoverType.cover_type_key)\n",
    "\n",
    "display(newdf.select('elevation','cover_type_label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d24752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "+--------------+-----------------+\n",
      "|cover_type_key| cover_type_label|\n",
      "+--------------+-----------------+\n",
      "|             1|       Spruce/Fir|\n",
      "|             2|   Lodgepole Pine|\n",
      "|             3|   Ponderosa Pine|\n",
      "|             4|Cottonwood/Willow|\n",
      "|             5|            Aspen|\n",
      "|             6|      Douglas-fir|\n",
      "|             7|        Krummholz|\n",
      "+--------------+-----------------+\n",
      "\n",
      "root\n",
      " |-- cover_type_key: string (nullable = true)\n",
      " |-- cover_type_label: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- cover_type_key: string (nullable = true)\n",
      " |-- cover_type_label: string (nullable = true)\n",
      " |-- cover_type_intkey: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# way 2\n",
    "\n",
    "df_coverType = spark.read.csv('covertype.csv', header=True)\n",
    "\n",
    "print(df_coverType.count())\n",
    "\n",
    "df_coverType.show()\n",
    "df_coverType.printSchema()\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_coverType = df_coverType.withColumn('cover_type_intkey', df_coverType['cover_type_key'].cast(IntegerType()))\n",
    "\n",
    "df_coverType.printSchema()\n",
    "\n",
    "df_coverType.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19df0bb",
   "metadata": {},
   "source": [
    "#Part 3: Data Ingestion, Cleansing, and Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a434c",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "\n",
    "This is a multi-step, data pipeline question in which you need to achieve a few objectives to build a successful job.\n",
    "\n",
    "### Data Sets\n",
    "\n",
    "#### `u.data.csv`\n",
    "\n",
    "- The full u data set, 100000 ratings by 943 users on 1682 items. \n",
    "- Each user has rated at least 20 movies.  \n",
    "- Users and items are numbered consecutively from 1. \n",
    "- The data is randomly ordered. \n",
    "- This is a tab separated file consisting of four columns: \n",
    "   - user id \n",
    "   - movie id \n",
    "   - rating \n",
    "   - date (unix seconds since 1/1/1970 UTC)\n",
    "\n",
    "#### Desired schema\n",
    "\n",
    "- `user_id INTEGER`\n",
    "- `movie_id INTEGER`\n",
    "- `rating INTEGER`\n",
    "- `date DATE `\n",
    "\n",
    "#### `u.item.csv`\n",
    "\n",
    "- This is a `|` separated file consisting of six columns:\n",
    "   - movie id\n",
    "   - movie title\n",
    "   - release date\n",
    "   - video release date\n",
    "   - IMDb URL\n",
    "   - genre\n",
    "- movie ids in this file match movie ids in `u.data`.\n",
    "\n",
    "#### Desired schema\n",
    "\n",
    "- `movie_id INTEGER`\n",
    "- `movie_title STRING`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b6288",
   "metadata": {},
   "source": [
    "### Question 9:  Code Challenge - Load DataFrames\n",
    "\n",
    "Use Apache Spark to perform the following:\n",
    "1. define the correct schemas for each Data Set to be imported as described above  \n",
    "   **note:** \n",
    "      - for `u.data.csv`, `date` *must* be stored using `DateType` with the format `yyyy-MM-dd`\n",
    "      - you may need to ingest `timestamp` data using `IntegerType`\n",
    "      - be sure to drop unneccesary columns for `u.item.csv`\n",
    "1. import the two files as DataFrames names `uDataDF` and `uItemDF` using the schemas you defined and these paths:\n",
    "   - `dbfs:/FileStore/tmp/u.data.csv`\n",
    "   - `dbfs:/FileStore/tmp/u.item.csv`\n",
    "1. order the `uDataDF` DataFrame by the `date` column\n",
    "\n",
    "**NOTE:** Please display the DataFrames, `uDataDF` and `uItemDF` after loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909ebfa",
   "metadata": {},
   "source": [
    "#### `uDataDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed0e8626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      "\n",
      "+-------+--------+------+----------+\n",
      "|user_id|movie_id|rating|   newdate|\n",
      "+-------+--------+------+----------+\n",
      "|    189|     732|     2|1998-04-23|\n",
      "|    653|     272|     4|1998-04-23|\n",
      "|    189|     381|     3|1998-04-23|\n",
      "+-------+--------+------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- newdate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# way 1\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "uDataDF = spark.read.load(\"u.data.csv\",\n",
    "                     format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"false\")\n",
    "\n",
    "uDataDF.printSchema()\n",
    "uDataDF = uDataDF.withColumnRenamed(\"_c0\", \"user_id\").withColumnRenamed(\"_c1\", \"movie_id\").withColumnRenamed(\"_c2\", \"rating\").withColumnRenamed(\"_c3\", \"date\").withColumn(\"tsDate\",from_unixtime(\"date\")).withColumn(\"newdate\",date_format('tsDate', \"yyyy-MM-dd\")).drop('tsDate','date')\n",
    "\n",
    "uDataDF.sort(desc(\"newdate\")).show(3)\n",
    "\n",
    "uDataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f8a08ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+--------------+\n",
      "|user_id|movie_id|rating|Converted_date|\n",
      "+-------+--------+------+--------------+\n",
      "|    259|     117|     4|    1997-09-20|\n",
      "|    119|     222|     5|    1997-09-20|\n",
      "|    259|     255|     4|    1997-09-20|\n",
      "+-------+--------+------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# way 2\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"movie_id\", IntegerType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"date\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "uDataDF = spark.read.csv('u.data.csv', inferSchema=True, sep='\\t', schema = schema)\n",
    "\n",
    "#uDataDF.show(3)\n",
    "\n",
    "#uDataDF.printSchema()\n",
    "\n",
    "\n",
    "uDataDF = uDataDF.select('user_id', 'movie_id', 'rating', to_date(from_unixtime(col('date'), 'yyyy-MM-dd')).alias('Converted_date'))\n",
    "\n",
    "#uDataDF.printSchema()\n",
    "\n",
    "#uDataDF.show(4)\n",
    "\n",
    "uDataDF.registerTempTable('uDataTB')\n",
    "\n",
    "spark.sql(\"select * from uDataTB order by Converted_date\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60d04e",
   "metadata": {},
   "source": [
    "#### `uItemDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb049c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+------------+------------------+--------------------+-----+\n",
      "|movie_id|      movie_title|release_date|video_release_date|            imdb_url|genre|\n",
      "+--------+-----------------+------------+------------------+--------------------+-----+\n",
      "|       1| Toy Story (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n",
      "|       2| GoldenEye (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n",
      "|       3|Four Rooms (1995)| 01-Jan-1995|              null|http://us.imdb.co...|    0|\n",
      "+--------+-----------------+------------+------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# way 1\n",
    "\n",
    "uItemDF = spark.read.load(\"u.item.csv\",\n",
    "                     format=\"csv\", sep=\"|\", inferSchema=\"true\", header=\"false\")\n",
    "\n",
    "uItemDF = uItemDF.select(\"_c0\",\"_c1\",\"_c2\",\"_c3\",\"_c4\",\"_c5\").withColumnRenamed(\"_c0\", \"movie_id\").withColumnRenamed(\"_c1\", \"movie_title\").withColumnRenamed(\"_c2\", \"release_date\").withColumnRenamed(\"_c3\", \"video_release_date\").withColumnRenamed(\"_c4\", \"imdb_url\").withColumnRenamed(\"_c5\", \"genre\")\n",
    "\n",
    "uItemDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa36e6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|movie_id|      movie_title|\n",
      "+--------+-----------------+\n",
      "|       1| Toy Story (1995)|\n",
      "|       2| GoldenEye (1995)|\n",
      "|       3|Four Rooms (1995)|\n",
      "+--------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#way 2\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"movie_id\", IntegerType(), True),\n",
    "    StructField(\"movie_title\", StringType(), True),\n",
    "])\n",
    "\n",
    "uItemDF = spark.read.csv('u.item.csv', inferSchema=True, sep='|', schema = schema)\n",
    "\n",
    "uItemDF.show(3)\n",
    "\n",
    "#uItemDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba796205",
   "metadata": {},
   "source": [
    "### Question 10:  Code Challenge - Perform a Join\n",
    "\n",
    "Use Apache Spark to do the following:\n",
    "- join `uDataDF` and `uItemDf` on `movie_id` as a new DataFrame called `uMovieDF`  \n",
    "   **note:** make sure you do not create duplicate `movie_id` columns\n",
    "   \n",
    "**NOTE:** Please display the DataFrame `uMovieDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88ed1d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+--------------+--------------------+\n",
      "|movie_id|user_id|rating|Converted_date|         movie_title|\n",
      "+--------+-------+------+--------------+--------------------+\n",
      "|     242|    196|     3|    1997-12-04|        Kolya (1996)|\n",
      "|     302|    186|     3|    1998-04-05|L.A. Confidential...|\n",
      "|     377|     22|     1|    1997-11-07| Heavyweights (1994)|\n",
      "+--------+-------+------+--------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uMovieDF = uDataDF.join(uItemDF,['movie_id'], \"inner\")\n",
    "uMovieDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49af642",
   "metadata": {},
   "source": [
    "### Question 11:  Code Challenge - Perform an Aggregation\n",
    "\n",
    "Use Apache Spark to do the following:\n",
    "1. create an aggregate DataFrame, `aggDF` by\n",
    "  1. extracting the year from the `date` (of the review)\n",
    "  1. getting the average rating of each film per year as a column named `average_rating`\n",
    "  1. ordering descending by year and average rating\n",
    "1. write the resulting dataframe to a table named \"movie_by_year_average_rating\" in the Default database  \n",
    "   **note:** use `mode(overwrite)` \n",
    "\n",
    "#### Desired Schema\n",
    "The schema of you resulting DataFrame should be:\n",
    "- `year INTEGER`\n",
    "- `movie_title STRING`\n",
    "- `average_rating DOUBLE`\n",
    "\n",
    "**NOTE:** Please display the DataFrame `aggDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "227e1db2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`newdate`' given input columns: [Converted_date, movie_id, movie_title, rating, user_id];\n'Project [movie_id#2283, user_id#2282, rating#2284, Converted_date#2290, movie_title#2622, year('newdate) AS year#2876]\n+- Project [movie_id#2283, user_id#2282, rating#2284, Converted_date#2290, movie_title#2622]\n   +- Join Inner, (movie_id#2283 = movie_id#2621)\n      :- Project [user_id#2282, movie_id#2283, rating#2284, to_date(from_unixtime('date, yyyy-MM-dd, None), None) AS Converted_date#2290]\n      :  +- Relation[user_id#2282,movie_id#2283,rating#2284,date#2285] csv\n      +- Relation[movie_id#2621,movie_title#2622] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-14de6cb5c262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0muMovieDF1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muMovieDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"newdate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   2453\u001b[0m         \"\"\"\n\u001b[0;32m   2454\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"col should be Column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2455\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2457\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\Spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`newdate`' given input columns: [Converted_date, movie_id, movie_title, rating, user_id];\n'Project [movie_id#2283, user_id#2282, rating#2284, Converted_date#2290, movie_title#2622, year('newdate) AS year#2876]\n+- Project [movie_id#2283, user_id#2282, rating#2284, Converted_date#2290, movie_title#2622]\n   +- Join Inner, (movie_id#2283 = movie_id#2621)\n      :- Project [user_id#2282, movie_id#2283, rating#2284, to_date(from_unixtime('date, yyyy-MM-dd, None), None) AS Converted_date#2290]\n      :  +- Relation[user_id#2282,movie_id#2283,rating#2284,date#2285] csv\n      +- Relation[movie_id#2621,movie_title#2622] csv\n"
     ]
    }
   ],
   "source": [
    "# way 1\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "uMovieDF1 = uMovieDF.withColumn(\"year\",year(\"newdate\"))\n",
    "\n",
    "\n",
    "windowSpec = Window.partitionBy(uMovieDF1['year'])\n",
    "cols = [\"year\",\"movie_title\"]\n",
    "\n",
    "\n",
    "uMovieDF1.withColumn('average_rating', avg(\"rating\").over(Window.partitionBy(cols))).drop(\"movie_id\",\"user_id\",\"rating\",\"newdate\",\"release_date\",\"video_release_date\",\"imdb_url\",\"genre\").distinct().show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5fba80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------------+\n",
      "|         movie_title|year|   average_rating|\n",
      "+--------------------+----+-----------------+\n",
      "|39 Steps, The (1935)|1998|              4.2|\n",
      "|39 Steps, The (1935)|1997|3.896551724137931|\n",
      "+--------------------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# way 2\n",
    "\n",
    "df = uMovieDF.withColumn('year', year('Converted_date'))\n",
    "\n",
    "df.registerTempTable('Movie')\n",
    "\n",
    "final_df = spark.sql('select movie_title, year, avg(rating) as average_rating from movie group by movie_title,year order by year desc, average_rating desc')\n",
    "\n",
    "final_df.filter(col('movie_title') == '39 Steps, The (1935)').show()\n",
    "\n",
    "final_df.registerTempTable('finalTB')\n",
    "\n",
    "#spark.sql('create table movie_by_year_average_rating as select * from finalTB')\n",
    "\n",
    "#hive = spark.HiveContext\n",
    "\n",
    "#final_df.write().mode(\"overwrite\").saveAsTable(\"movie_by_year_average_rating\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5196345",
   "metadata": {},
   "source": [
    "## Part 4: Fun with JSON\n",
    "\n",
    "JSON values are typically passed by message brokers such as Kafka or Kinesis in a string encoding. When consumed by a Spark Structured Streaming application, this json must be converted into a nested object in order to be used.\n",
    "\n",
    "Below is a list of json strings that represents how data might be passed from a message broker.\n",
    "\n",
    "**Note:** Make sure to run the cell below to retrieve the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27774ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "\n",
    "\n",
    "sampleJson =  [{\n",
    " \t\t\"user\": 100,\n",
    " \t\t\"ips\": [\"191.168.192.101\", \"191.168.192.103\", \"191.168.192.96\", \"191.168.192.99\"]\n",
    " \t},\n",
    " \t{\n",
    " \t\t\"user\": 101,\n",
    " \t\t\"ips\": [\"191.168.192.102\", \"191.168.192.105\", \"191.168.192.103\", \"191.168.192.107\"]\n",
    " \t},\n",
    " \t{\n",
    " \t\t\"user\": 102,\n",
    " \t\t\"ips\": [\"191.168.192.105\", \"191.168.192.101\", \"191.168.192.105\", \"191.168.192.107\"]\n",
    " \t},\n",
    " \t{\n",
    " \t\t\"user\": 103,\n",
    " \t\t\"ips\": [\"191.168.192.96\", \"191.168.192.100\", \"191.168.192.107\", \"191.168.192.101\"]\n",
    " \t},\n",
    " \t{\n",
    " \t\t\"user\": 104,\n",
    " \t\t\"ips\": [\"191.168.192.99\", \"191.168.192.99\", \"191.168.192.102\", \"191.168.192.99\"]\n",
    " \t},\n",
    " \t{\n",
    " \t\t\"user\": 105,\n",
    " \t\t\"ips\": [\"191.168.192.99\", \"191.168.192.99\", \"191.168.192.100\", \"191.168.192.96\"]\n",
    " \t}\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afec93b",
   "metadata": {},
   "source": [
    "### Question 12:  Code Challenge - Count the IPs\n",
    "\n",
    "Use any coding techniques known to you to parse this list of JSON strings to answer the following question:\n",
    "- how many occurrences of each IP address are in this list?\n",
    "\n",
    "#### Desired Output\n",
    "Your results should be this:\n",
    "\n",
    "\n",
    "| ip | count |\n",
    "|:-:|:-:|\n",
    "| `191.168.192.96` | `3` |\n",
    "| `191.168.192.99` | `6` |\n",
    "| `191.168.192.100` | `2` |\n",
    "| `191.168.192.101` | `3` |\n",
    "| `191.168.192.102` | `2` |\n",
    "| `191.168.192.103` | `2` |\n",
    "| `191.168.192.105` | `3` |\n",
    "| `191.168.192.107` | `3` |\n",
    "\n",
    "**NOTE:** The order of your results is not important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24a34d60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampleJson' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-06f397c1c53b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mjsondf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multiLine'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'true'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampleJson\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mjsondf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ips\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"col\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'col'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"col\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sampleJson' is not defined"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "jsondf = spark.read.option('multiLine','true').json(sc.parallelize(sampleJson))\n",
    "jsondf.select(explode(col(\"ips\"))).groupBy(\"col\").agg(count('col')).withColumnRenamed(\"col\",\"ip\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9aa807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
