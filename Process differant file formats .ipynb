{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e3546b",
   "metadata": {},
   "source": [
    "# Q.1 Process JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea833888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e147c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Assuming JAVA_HOME is set in enviroment variables  \n",
    "#os.environ[\"SPARK_HOME\"] = r\"C:\\Users\\rosha\\Downloads\\Spark\\spark-3.1.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b38aa6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b44060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Spark application\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cdcb1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-9TN2O3PF.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x26f8a973b00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f3cc2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StringType, StructField, StructType, BooleanType, ArrayType, IntegerType\n",
    "'''\n",
    "schema = StructType([\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"wins\", ArrayType(\n",
    "      StructType([\n",
    "          StructField(\"Type\", StringType()),\n",
    "          StructField(\"Award\", StringType()),\n",
    "      ])\n",
    "    ),True)\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fe0c189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.schema(schema).option(\"multiline\",\"true\").json(r\"data\\sample.json\")\n",
    "df = spark.read.json(r\"data\\sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cdee673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|   name|                wins|\n",
      "+-------+--------------------+\n",
      "|Gilbert|[[straight, 7♣], ...|\n",
      "+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "362e1c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|   name|                wins|\n",
      "+-------+--------------------+\n",
      "|Gilbert|[[straight, 7♣], ...|\n",
      "|  Alexa|[[two pair, 4♠], ...|\n",
      "|    May|                  []|\n",
      "|Deloise|[[three of a kind...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "139b80fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- wins: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b539b",
   "metadata": {},
   "source": [
    "### Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9268157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+\n",
      "|name   |col                  |\n",
      "+-------+---------------------+\n",
      "|Gilbert|[straight, 7♣]       |\n",
      "|Gilbert|[one pair, 10♥]      |\n",
      "|Alexa  |[two pair, 4♠]       |\n",
      "|Alexa  |[two pair, 9♠]       |\n",
      "|Deloise|[three of a kind, 5♣]|\n",
      "+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select(df.name, explode(df.wins)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9ea2f",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "983a279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------+\n",
      "|name   |flatten(wins)                |\n",
      "+-------+-----------------------------+\n",
      "|Gilbert|[straight, 7♣, one pair, 10♥]|\n",
      "|Alexa  |[two pair, 4♠, two pair, 9♠] |\n",
      "|May    |[]                           |\n",
      "|Deloise|[three of a kind, 5♣]        |\n",
      "+-------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import flatten\n",
    "\n",
    "df.select(df.name, flatten(df.wins)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc3d06",
   "metadata": {},
   "source": [
    "# Q.2 Process Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8eb25bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = spark.read.text(r\"C:\\Users\\rosha\\Downloads\\Spark\\data\\gsk.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bc0dbd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+\n",
      "|value                                                                       |\n",
      "+----------------------------------------------------------------------------+\n",
      "|03/22 08:51:01 INFO :..settcpimage: Associate with TCP/IP image name = TCPCS|\n",
      "|03/22 08:51:02 INFO :..reg_process: registering process with the system     |\n",
      "|03/22 08:51:02 INFO :..reg_process: attempt OS/390 registration             |\n",
      "|03/22 08:51:02 INFO :..reg_process: return from registration rc=0           |\n",
      "+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "83010148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e896374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---------------+----------------------------------------+\n",
      "|timestamp     |level|object         |text                                    |\n",
      "+--------------+-----+---------------+----------------------------------------+\n",
      "|03/22 08:51:01|INFO |:..settcpimage:|Associate with TCP/IP image name = TCPCS|\n",
      "|03/22 08:51:02|INFO |:..reg_process:|registering process with the system     |\n",
      "|03/22 08:51:02|INFO |:..reg_process:|attempt OS/390 registration             |\n",
      "|03/22 08:51:02|INFO |:..reg_process:|return from registration rc=0           |\n",
      "+--------------+-----+---------------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, regexp_extract\n",
    "'''\n",
    "split_df = log_df.select(regexp_extract('value', r'^(\\d{2}/\\d{2}\\s+\\d{2}:\\d{2}:\\d{2})', 1).alias('timestamp')\n",
    "                        ,regexp_extract('value', r'^(.*\\s+)(\\w{4})(\\s+):', 2).alias('level')\n",
    "                        ,regexp_extract('value', r'^(.*\\s+)(:..\\w+:)', 2).alias('object')\n",
    "                        ,regexp_extract('value', r'^.*\\s+:..\\w+:\\s+(\\w+.*$)', 1).alias('text')\n",
    "                        )\n",
    "'''\n",
    "split_df = log_df.select(regexp_extract('value', r'^(.*)\\s+(\\w+)\\s+(:..\\w+:)\\s+(\\w+.*)', 1).alias('timestamp')\n",
    "                        ,regexp_extract('value', r'^(.*)\\s+(\\w+)\\s+(:..\\w+:)\\s+(\\w+.*)', 2).alias('level')\n",
    "                        ,regexp_extract('value', r'^(.*)\\s+(\\w+)\\s+(:..\\w+:)\\s+(\\w+.*)', 3).alias('object')\n",
    "                        ,regexp_extract('value', r'^(.*)\\s+(\\w+)\\s+(:..\\w+:)\\s+(\\w+.*)', 4).alias('text')\n",
    "                        )\n",
    "\n",
    "#r'^(\\d{4}-\\d{2}-\\d{2})(\\s+)(\\d{2}:\\d{2}:\\d{2})'\n",
    "#(\\s+) - One or many spaces\n",
    "#(.*) - Any Character with Zero or more repetitions\n",
    "#\\d{2} - Any digit with m Repetitions\n",
    "split_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b919321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "01d32f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------------+--------------------+\n",
      "|               Date|level|         object|                text|\n",
      "+-------------------+-----+---------------+--------------------+\n",
      "|2022-03-01 08:51:01| INFO|:..settcpimage:|Associate with TC...|\n",
      "|2022-03-01 08:51:02| INFO|:..reg_process:|registering proce...|\n",
      "|2022-03-01 08:51:02| INFO|:..reg_process:|attempt OS/390 re...|\n",
      "|2022-03-01 08:51:02| INFO|:..reg_process:|return from regis...|\n",
      "+-------------------+-----+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "log_df = split_df.select(to_timestamp(split_df.timestamp, 'MM/yy HH:mm:ss').alias('Date'),'level','object','text')\n",
    "log_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "97f50203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "abc…\t      Letters\n",
    "123…\t      Digits\n",
    "\\d\t          Any Digit\n",
    "\\D\t          Any Non-digit character\n",
    ".\t          Any Character\n",
    "\\.\t          Period\n",
    "[abc]\t      Only a, b, or c\n",
    "[^abc]\t      Not a, b, nor c\n",
    "[a-z]\t      Characters a to z\n",
    "[0-9]\t      Numbers 0 to 9\n",
    "\\w\t          Any Alphanumeric character\n",
    "\\W\t          Any Non-alphanumeric character\n",
    "{m}\t          m Repetitions\n",
    "{m,n}\t      m to n Repetitions\n",
    "*\t          Zero or more repetitions\n",
    "+\t          One or more repetitions\n",
    "?\t          Optional character\n",
    "\\s\t          Any Whitespace\n",
    "\\S\t          Any Non-whitespace character\n",
    "^…$\t          Starts and ends\n",
    "(…)\t          Capture Group\n",
    "(a(bc))\t      Capture Sub-group\n",
    "(.*)\t      Capture all6-\n",
    "(abc|def)\t  Matches abc or def\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "^            Assert position at the beginning of the line\n",
    "(\\w+)        Capture one or more word characters (a-zA-Z0-9_) into group 1\n",
    "[ \\t]*       Match any number of spaces or tab characters ([ \\t] can be replaced with \\h in some regex flavours such as PCRE)\n",
    ".*           Match any character (except newline unless the s modifier is used)\n",
    "\\bby         Match a word boundary \\b, followed by by literally\n",
    "[ \\t]+       Match one or more spaces or tab characters\n",
    "(\\w+)        Capture one or more word characters (a-zA-Z0-9_) into group 3\n",
    "[ \\t]*       Match any number of spaces or tab characters\n",
    ".*           Match any character any number of times\n",
    "$            Assert position at the end of the line\n",
    "\n",
    "(.*\\bby[ \\t]+(\\w+)[ \\t]*.*)        Capture the following into group 2\n",
    "\n",
    "'''\n",
    "\n",
    "#https://regexone.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "73a9ed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  ID|               Notes|\n",
      "+----+--------------------+\n",
      "|2345|     Checked by John|\n",
      "|2398|   Verified by Stacy|\n",
      "|2328|Verified by Srini...|\n",
      "|3983|Double Checked on...|\n",
      "+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('2345', 'Checked by John'),\n",
    "('2398', 'Verified by Stacy'),\n",
    "('2328', 'Verified by Srinivas than some random text'),        \n",
    "('3983', 'Double Checked on 2/23/17 by Marsha')]\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = sc.parallelize(data).toDF(['ID', 'Notes'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "700ef053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------+\n",
      "|  ID|               Notes|Employee|\n",
      "+----+--------------------+--------+\n",
      "|2345|     Checked by John|    John|\n",
      "|2398|   Verified by Stacy|   Stacy|\n",
      "|2328|Verified by Srini...|Srinivas|\n",
      "|3983|Double Checked on...|  Marsha|\n",
      "+----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "result = df.withColumn('Employee', regexp_extract(col('Notes'), '(.)(by)(\\s+)(\\w+)', 4))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b339367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
